{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec4a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu==1.7.4\n",
      "  Using cached faiss-cpu-1.7.4.tar.gz (57 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: mistralai in c:\\users\\priya\\anaconda3\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: httpx<1,>=0.25 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: orjson<3.11,>=3.9.10 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from mistralai) (3.10.18)\n",
      "Requirement already satisfied: pydantic<3,>=2.5.2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from mistralai) (2.11.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.25->mistralai) (1.3.0)\n",
      "Building wheels for collected packages: faiss-cpu\n",
      "  Building wheel for faiss-cpu (pyproject.toml): started\n",
      "  Building wheel for faiss-cpu (pyproject.toml): finished with status 'error'\n",
      "Failed to build faiss-cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for faiss-cpu (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      C:\\Users\\priya\\AppData\\Local\\Temp\\pip-build-env-7v2nsem0\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: MIT License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      running build_ext\n",
      "      building 'faiss._swigfaiss' extension\n",
      "      swigging faiss\\faiss\\python\\swigfaiss.i to faiss\\faiss\\python\\swigfaiss_wrap.cpp\n",
      "      swig.exe -python -c++ -Doverride= -I/usr/local/include -Ifaiss -doxygen -DSWIGWIN -module swigfaiss -o faiss\\faiss\\python\\swigfaiss_wrap.cpp faiss\\faiss\\python\\swigfaiss.i\n",
      "      error: command 'swig.exe' failed: None\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for faiss-cpu\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (faiss-cpu)\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu==1.7.4 mistralai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46b0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "api_key = getpass(\"Type your API Key\")\n",
    "client = MistralClient(api_key=api_key)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aac877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f59086",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('essay.txt','w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0374c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75014"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6f63fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0,len(text), chunk_size)]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33853c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "          model=\"mistral-embed\",\n",
    "          inputs=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5a4af3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_embedding(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks])\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     embeddings_batch_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      3\u001b[0m           model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m           inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\n\u001b[0;32m      5\u001b[0m       )\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_batch_response\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "text_embeddings = np.array([get_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aaf82ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text_embeddings\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ded33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03979492,  0.07733154,  0.00013709, ..., -0.01274109,\n",
       "        -0.02101135, -0.00264168],\n",
       "       [-0.03152466,  0.07226562,  0.02961731, ..., -0.01079559,\n",
       "        -0.01189423, -0.00821686],\n",
       "       [-0.05905151,  0.06112671,  0.01206207, ..., -0.0226593 ,\n",
       "         0.00488663, -0.00665283],\n",
       "       ...,\n",
       "       [-0.05477905,  0.06890869,  0.02703857, ..., -0.02456665,\n",
       "        -0.02526855, -0.02687073],\n",
       "       [-0.03884888,  0.05587769,  0.04718018, ..., -0.01812744,\n",
       "         0.00926208, -0.00866699],\n",
       "       [-0.03048706,  0.05831909,  0.01704407, ..., -0.01620483,\n",
       "        -0.01800537, -0.04415894]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63104311",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = text_embeddings.shape[1]\n",
    "Index = faiss.IndexFlatL2(d)\n",
    "Index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eec73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What were the two main things the author worked on before college?\"\n",
    "question_embeddings = np.array([get_embeddings(question)])\n",
    "question_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25fce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05447388,  0.03479004,  0.0375061 , ..., -0.02787781,\n",
       "        -0.00327492,  0.0029068 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94102e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3]]\n"
     ]
    }
   ],
   "source": [
    "D, I = Index.search(question_embeddings, k =2)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec8176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on ', 'arvard accepted me, so that was where I went.\\n\\nI don\\'t remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that\\'s told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.\\n\\nWhat these programs really showed was that there\\'s a subset of natural language that\\'s a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. Its brokenness did, as so often happens, generate a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to get us Mike.\\n\\nSo I looked around to see what I could salvage from the wreckage of my plans, and there was Lisp. I knew from experience that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp hacking. It\\'s scary to think how little I knew about Lisp hacking when I started writing that book. But there\\'s nothing like writing a book about something to help you learn it. The book, On Lisp, wasn\\'t published till 1993, but I wrote much of it in grad school.\\n\\nComputer Science is an uneasy alliance between two halves, theory and systems. The theory people prove things, and the systems people build things. I wanted to build things. I had plenty of respect for theory — indeed, a sneaking suspicion that it was the more admirable of the two halves — but building things seemed so much more exciting.\\n\\nThe problem with systems work, though, was that it didn\\'t last. ']\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def run_mistral(user_message, model = \"mistral-large-latest\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\" : \"user\", \"content\" : user_message\n",
    "        }\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9aece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two main things the author worked on before college were writing and programming.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_mistral(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b3b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priya\\anaconda3\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:186: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two main things the author worked on before college were writing and programming. He wrote short stories and tried writing programs on an IBM 1401 using an early version of Fortran.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "load = TextLoader(\"essay.txt\")\n",
    "docs = load.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = MistralAIEmbeddings(model = \"mistral-embed\", mistral_api_key = api_key)\n",
    "\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "model = ChatMistralAI(mistral_api_key = api_key)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Answer the following question based only on the provided context:\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index==0.10.55\n",
      "  Downloading llama_index-0.10.55-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-llms-mistralai==0.1.18\n",
      "  Downloading llama_index_llms_mistralai-0.1.18-py3-none-any.whl.metadata (679 bytes)\n",
      "Collecting llama-index-embeddings-mistralai\n",
      "  Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl.metadata (696 bytes)\n",
      "Collecting mistralai==0.4.2\n",
      "  Downloading mistralai-0.4.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core==0.10.55 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_core-0.10.55-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.10-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index==0.10.55)\n",
      "  Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from mistralai==0.4.2) (0.28.1)\n",
      "Requirement already satisfied: orjson<3.11,>=3.9.10 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from mistralai==0.4.2) (3.10.18)\n",
      "Requirement already satisfied: pydantic<3,>=2.5.2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from mistralai==0.4.2) (2.11.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.55->llama-index==0.10.55) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (0.6.7)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.55->llama-index==0.10.55)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.55->llama-index==0.10.55)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (2024.6.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (1.93.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.55->llama-index==0.10.55)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-core==0.10.55->llama-index==0.10.55) (1.14.1)\n",
      "INFO: pip is looking at multiple versions of llama-index-embeddings-mistralai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-embeddings-mistralai\n",
      "  Downloading llama_index_embeddings_mistralai-0.2.0-py3-none-any.whl.metadata (696 bytes)\n",
      "  Downloading llama_index_embeddings_mistralai-0.1.6-py3-none-any.whl.metadata (696 bytes)\n",
      "  Downloading llama_index_embeddings_mistralai-0.1.5-py3-none-any.whl.metadata (645 bytes)\n",
      "  Downloading llama_index_embeddings_mistralai-0.1.4-py3-none-any.whl.metadata (645 bytes)\n",
      "Requirement already satisfied: anyio in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai==0.4.2) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai==0.4.2) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai==0.4.2) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25->mistralai==0.4.2) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai==0.4.2) (0.14.0)\n",
      "Collecting llama-cloud==0.1.32 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.32-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.9-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.30 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.30-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.6-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.5-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.25 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.25-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.23 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.23-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.21 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.21-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.34-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_llms_openai-0.1.30-py3-none-any.whl.metadata (650 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.28-py3-none-any.whl.metadata (650 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.26-py3-none-any.whl.metadata (610 bytes)\n",
      "INFO: pip is looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.55) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.55)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.55)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-readers-llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index==0.10.55)\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.51-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai==0.4.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai==0.4.2) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.5.2->mistralai==0.4.2) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index==0.10.55) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index==0.10.55) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index==0.10.55) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index==0.10.55) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index==0.10.55) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index==0.10.55) (1.11.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.55) (2.5)\n",
      "Collecting llama-cloud-services>=0.6.51 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.51-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: click in c:\\users\\priya\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama-index==0.10.55) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\priya\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama-index==0.10.55) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama-index==0.10.55) (2024.9.11)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-core==0.10.55->llama-index==0.10.55) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-core==0.10.55->llama-index==0.10.55) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\priya\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-core==0.10.55->llama-index==0.10.55) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.10.55->llama-index==0.10.55) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.10.55->llama-index==0.10.55) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.55->llama-index==0.10.55) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\priya\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core==0.10.55->llama-index==0.10.55) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.55->llama-index==0.10.55) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core==0.10.55->llama-index==0.10.55) (3.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pandas->llama-index-core==0.10.55->llama-index==0.10.55) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pandas->llama-index-core==0.10.55->llama-index==0.10.55) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from pandas->llama-index-core==0.10.55->llama-index==0.10.55) (2023.3)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.50-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.49 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.50-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading llama_cloud_services-0.6.49-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.49-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.6.48-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.48 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.48-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.47-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting llama-cloud-services>=0.6.47 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.47-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.33-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.46-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.45 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.46-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading llama_cloud_services-0.6.45-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.45-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.6.44-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.44 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.44-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.43-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.43 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.43-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.42-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.42 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.42-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.41-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.41 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.41-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.40-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.40 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.40-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.39-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.39 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.39-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.38-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.37 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.38-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.29-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-cloud-services>=0.6.37 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.37-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.37-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.6.36-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.36 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.36-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.28-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.35-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.35 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.35-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.27-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.34-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.32 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.34-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.33-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.32-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.33-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.6.32-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.6.31-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.31 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.31-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.30-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.30 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.30-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.28-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.28 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.29-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.28-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.27-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.27 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.27-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.26-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.26 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.26-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.25-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.24 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.25-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.24-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.24-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.6.23-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.23 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.23-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.22-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.22-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.22 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.22-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.55)\n",
      "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.21-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.21 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.21-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.20-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.20 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.20-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.18-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.17 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.19-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.18-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.17-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.16-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.16 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.16-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Using cached llama_parse-0.6.12-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.12 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Using cached llama_cloud_services-0.6.15-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.14-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading llama_cloud_services-0.6.12-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.9 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.11-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading llama_cloud_services-0.6.10-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading llama_cloud_services-0.6.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Using cached llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached llama_cloud_services-0.6.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading llama_cloud_services-0.6.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading llama_cloud_services-0.6.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.3 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.2 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.1 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting llama-cloud-services (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_cloud_services-0.6.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.55)\n",
      "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.11-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.55->llama-index==0.10.55) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.55->llama-index==0.10.55) (1.16.0)\n",
      "Downloading llama_index-0.10.55-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_llms_mistralai-0.1.18-py3-none-any.whl (5.6 kB)\n",
      "Downloading mistralai-0.4.2-py3-none-any.whl (20 kB)\n",
      "Downloading llama_index_core-0.10.55-py3-none-any.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.5 MB 3.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.3/15.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 3.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.1/15.5 MB 4.0 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.0/15.5 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/15.5 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.2/15.5 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.0/15.5 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_mistralai-0.1.4-py3-none-any.whl (2.6 kB)\n",
      "Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.1.26-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_cloud-0.1.34-py3-none-any.whl (289 kB)\n",
      "Downloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, pypdf, deprecated, tiktoken, mistralai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: mistralai\n",
      "    Found existing installation: mistralai 1.9.3\n",
      "    Uninstalling mistralai-1.9.3:\n",
      "      Successfully uninstalled mistralai-1.9.3\n",
      "Successfully installed deprecated-1.2.18 dirtyjson-1.0.8 llama-cloud-0.1.34 llama-index-0.10.55 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.55 llama-index-embeddings-mistralai-0.1.4 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48.post4 llama-index-llms-mistralai-0.1.18 llama-index-llms-openai-0.1.26 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 mistralai-0.4.2 pypdf-4.3.1 striprtf-0.0.26 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index==0.10.55 llama-index-llms-mistralai==0.1.18 llama-index-embeddings-mistralai mistralai==0.4.2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fb6b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What were the two main things the author worked on before college?', 'context': [Document(id='229a0367-5c56-4faa-bf03-af13cfa5f9c3', metadata={'source': 'essay.txt'}, page_content='What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.'), Document(id='4f8c5444-6022-496e-bed1-d756cb54a76f', metadata={'source': 'essay.txt'}, page_content='While I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny, because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still lives is different from painting people, because the subject, as its name suggests, can\\'t move. People can\\'t sit for more than about 15 minutes at a time, and when they do they don\\'t sit very still. So the traditional m.o. for painting people is to know how to paint a generic person, which you then modify to match the specific person you\\'re painting. Whereas a still life you can, if you want, copy pixel by pixel from what you\\'re seeing. You don\\'t want to stop there, of course, or you get merely photographic accuracy, and what makes a still life interesting is that it\\'s been through a head. You want to emphasize the visual cues that tell you, for example, that the reason the color changes suddenly at a certain point is that it\\'s the edge of an object. By subtly emphasizing such things you can make paintings that are more realistic than photographs not just in some metaphorical sense, but in the strict information-theoretic sense. [4]\\n\\nI liked painting still lives because I was curious about what I was seeing. In everyday life, we aren\\'t consciously aware of much we\\'re seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn\\'t teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.'), Document(id='116af7ef-662b-4e95-9edc-709d34cc53b4', metadata={'source': 'essay.txt'}, page_content=\"I've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\\n\\nI knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.\\n\\nOne of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm writing, and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious intellectually in something like the way Latin is, also seems about as hip.\\n\\nIt's not that unprestigious types of work are good per se. But when you find yourself drawn to some kind of work despite its current lack of prestige, it's a sign both that there's something real to be discovered there, and that you have the right kind of motives. Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees you're not on the most common type of wrong one.\\n\\nOver the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\\n\\nOne night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\\n\\nJessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\\n\\nWhen the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.\\n\\nOne of my tricks for writing essays had always been to give talks. The prospect of having to stand up in front of a group of people and tell them something that won't waste their time is a great spur to the imagination. When the Harvard Computer Society, the undergrad computer club, asked me to give a talk, I decided I would tell them how to start a startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.\"), Document(id='c283e135-5f2f-4009-a1b5-fb6bc8743d57', metadata={'source': 'essay.txt'}, page_content='Then one day in April 1990 a crack appeared in the wall. I ran into professor Cheatham and he asked if I was far enough along to graduate that June. I didn\\'t have a word of my dissertation written, but in what must have been the quickest bit of thinking in my life, I decided to take a shot at writing one in the 5 weeks or so that remained before the deadline, reusing parts of On Lisp where I could, and I was able to respond, with no perceptible delay \"Yes, I think so. I\\'ll give you something to read in a few days.\"\\n\\nI picked applications of continuations as the topic. In retrospect I should have written about macros and embedded languages. There\\'s a whole world there that\\'s barely been explored. But all I wanted was to get out of grad school, and my rapidly written dissertation sufficed, just barely.\\n\\nMeanwhile I was applying to art schools. I applied to two: RISD in the US, and the Accademia di Belli Arti in Florence, which, because it was the oldest art school, I imagined would be good. RISD accepted me, and I never heard back from the Accademia, so off to Providence I went.\\n\\nI\\'d applied for the BFA program at RISD, which meant in effect that I had to go to college again. This was not as strange as it sounds, because I was only 25, and art schools are full of people of different ages. RISD counted me as a transfer sophomore and said I had to do the foundation that summer. The foundation means the classes that everyone has to take in fundamental subjects like drawing, color, and design.\\n\\nToward the end of the summer I got a big surprise: a letter from the Accademia, which had been delayed because they\\'d sent it to Cambridge England instead of Cambridge Massachusetts, inviting me to take the entrance exam in Florence that fall. This was now only weeks away. My nice landlady let me leave my stuff in her attic. I had some money saved from consulting work I\\'d done in grad school; there was probably enough to last a year if I lived cheaply. Now all I had to do was learn Italian.\\n\\nOnly stranieri (foreigners) had to take this entrance exam. In retrospect it may well have been a way of excluding them, because there were so many stranieri attracted by the idea of studying art in Florence that the Italian students would otherwise have been outnumbered. I was in decent shape at painting and drawing from the RISD foundation that summer, but I still don\\'t know how I managed to pass the written exam. I remember that I answered the essay question by writing about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited vocabulary. [2]\\n\\nI\\'m only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed. The students and faculty in the painting department at the Accademia were the nicest people you could imagine, but they had long since arrived at an arrangement whereby the students wouldn\\'t require the faculty to teach anything, and in return the faculty wouldn\\'t require the students to learn anything. And at the same time all involved would adhere outwardly to the conventions of a 19th century atelier. We actually had one of those little stoves, fed with kindling, that you see in 19th century studio paintings, and a nude model sitting as close to it as possible without getting burned. Except hardly anyone else painted her besides me. The rest of the students spent their time chatting or occasionally trying to imitate things they\\'d seen in American art magazines.\\n\\nOur model turned out to live just down the street from me. She made a living from a combination of modelling and making fakes for a local antique dealer. She\\'d copy an obscure old painting out of a book, and then he\\'d take the copy and maltreat it to make it look old. [3]')], 'answer': 'The two main things the author worked on before college were writing and programming. He wrote short stories and tried writing programs on an IBM 1401 using an early version of Fortran.'}\n"
     ]
    }
   ],
   "source": [
    "#llamaindex\n",
    "\n",
    "import os\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=['essay.txt'])\n",
    "documents = reader.load_data()\n",
    "\n",
    "Settings.llm = MistralAI(model=\"mistral-medium\", api_key= api_key)\n",
    "Settings.embed_model = MistralAIEmbedding(model_name=\"mistral-embed\", api_key=api_key)\n",
    "\n",
    "Index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = Index.as_query_engine(similarity_top_k = 2)\n",
    "reponse = query_engine.query(\n",
    "    \"What were the two main things the author worked on before college?\"\n",
    ")\n",
    "print(str(response))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
