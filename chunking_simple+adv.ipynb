{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqzAv+Uffezm1/F/YavbC3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyal6/gen/blob/main/chunking_simple%2Badv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXClq1uwBe04",
        "outputId": "2e70cee1-c87d-45a3-adfd-3a577c2f6adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Fixed-size chunking is the simplest approach. It splits text into\n",
            "\n",
            "Chunk 2:\n",
            "text into chunks of a predetermined size, often measured in tokens or\n",
            "\n",
            "Chunk 3:\n",
            "tokens or characters. Chunk overlap preserves context between boundaries.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#fixed size chunking\n",
        "from typing import List\n",
        "import re\n",
        "\n",
        "\n",
        "def word_splitter(source_text: str) ->List[str]:\n",
        "\n",
        "  source_text = re.sub(r\"\\s+\", \" \", source_text.strip())\n",
        "  return source_text.split(\" \")\n",
        "\n",
        "def get_chunks_fixed_size_with_overlap(\n",
        "    text:str, chunk_size:int, overlap_fraction: float = 0.2) -> List[str]:\n",
        "    words = word_splitter(text)\n",
        "    overlap = int(chunk_size * overlap_fraction)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "      start = max(i-overlap, 0)\n",
        "      end = i + chunk_size\n",
        "      chunk = \" \".join(words[start:end])\n",
        "      chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "text = \"\"\"Fixed-size chunking is the simplest approach.\n",
        "It splits text into chunks of a predetermined size, often measured in tokens or characters.\n",
        "Chunk overlap preserves context between boundaries.\"\"\"\n",
        "\n",
        "chunks = get_chunks_fixed_size_with_overlap(text, chunk_size=10, overlap_fraction=0.2)\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"Chunk {i}:\\n{chunk}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#recursive chunking\n",
        "from typing import List\n",
        "\n",
        "def recursive_chunking(text: str, max_chunk_size: int = 1000) -> List[str]:\n",
        "  if len(text) <= max_chunk_size:\n",
        "    return [text.strip()] if text.strip() else []\n",
        "\n",
        "  separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
        "\n",
        "  for sep in separators:\n",
        "    if sep in text:\n",
        "      parts = text.split(sep)\n",
        "      chunks = []\n",
        "      current_chunk = \"\"\n",
        "\n",
        "      for part in parts:\n",
        "\n",
        "        test_chunk = current_chunk + sep + part if current_chunk else part\n",
        "\n",
        "        if len(test_chunk) <=max_chunk_size:\n",
        "          current_chunk = test_chunk\n",
        "        else:\n",
        "          if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "          current_chunk = part\n",
        "\n",
        "      if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "      final_chunks = []\n",
        "      for chunk in chunks:\n",
        "        if len(chunk) > max_chunk_size:\n",
        "          final_chunks.extend(recursive_chunking(chunk, max_chunk_size))\n",
        "        else:\n",
        "          final_chunks.append(chunk)\n",
        "      return [chunk for chunk in final_chunks if chunk]\n",
        "\n",
        "  return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "\n",
        "text = \"\"\"Recursive chunking is a more nuanced approach.\n",
        "It splits text using a prioritized list of separators, like paragraphs or sentences.\n",
        "If a piece of text is still too big, it splits it again until it's small enough.\"\"\"\n",
        "\n",
        "chunks = recursive_chunking(text, max_chunk_size=60)\n",
        "for i, c in enumerate(chunks, 1):\n",
        "    print(f\"Chunk {i}:\\n{c}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn377IlQdsov",
        "outputId": "9b2e4b96-12e3-4f93-94b2-682c2a0115ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Recursive chunking is a more nuanced approach.\n",
            "\n",
            "Chunk 2:\n",
            "It splits text using a prioritized list of separators, like\n",
            "\n",
            "Chunk 3:\n",
            "paragraphs or sentences.\n",
            "\n",
            "Chunk 4:\n",
            "If a piece of text is still too big, it splits it again\n",
            "\n",
            "Chunk 5:\n",
            "until it's small enough.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#document based chunking\n",
        "from typing import List\n",
        "import re\n",
        "\n",
        "\n",
        "def markdown_document_chunking(text:str) -> List[str]:\n",
        "  header_pattern = r'^#{1,6}\\s+.+$'\n",
        "  lines = text.split(\"\\n\")\n",
        "\n",
        "  chunks = []\n",
        "  current_chunk = []\n",
        "\n",
        "  for line in lines:\n",
        "    if re.match(header_pattern, line, re.MULTILINE):\n",
        "\n",
        "      if current_chunk:\n",
        "        chunk_text = '\\n'.join(current_chunk).strip()\n",
        "        # Append the previous chunk before starting a new one\n",
        "        if chunk_text:\n",
        "          chunks.append(chunk_text)\n",
        "      current_chunk = [line]\n",
        "    else:\n",
        "      current_chunk.append(line)\n",
        "\n",
        "  if current_chunk:\n",
        "     chunk_text = '\\n'.join(current_chunk).strip()\n",
        "     if chunk_text:\n",
        "      chunks.append(chunk_text)\n",
        "  return chunks\n",
        "text = \"\"\"\n",
        "# Introduction\n",
        "This is the intro section.\n",
        "\n",
        "## Background\n",
        "Some background information.\n",
        "\n",
        "## Methods\n",
        "Details about methods used.\n",
        "\n",
        "# Conclusion\n",
        "Final thoughts and summary.\n",
        "\"\"\"\n",
        "\n",
        "chunks = markdown_document_chunking(text)\n",
        "for i, c in enumerate(chunks, 1):\n",
        "    print(f\"Chunk {i}:\\n{c}\\n\")"
      ],
      "metadata": {
        "id": "9vMuR7A-mNfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09f8879-3fd1-440b-fc6c-ffdd3c4b3fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "# Introduction\n",
            "This is the intro section.\n",
            "\n",
            "Chunk 2:\n",
            "## Background\n",
            "Some background information.\n",
            "\n",
            "Chunk 3:\n",
            "## Methods\n",
            "Details about methods used.\n",
            "\n",
            "Chunk 4:\n",
            "# Conclusion\n",
            "Final thoughts and summary.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#semantic chunking\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def semantic_chunking(text:str, threshold: float = 0.7, max_sentences: int = 5):\n",
        "  model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  sentences = text.split('. ')\n",
        "  embeddings = model.encode(sentences)\n",
        "\n",
        "  chunks=[]\n",
        "  current_chunk = [sentences[0]]\n",
        "\n",
        "  for i in range(1, len(sentences)):\n",
        "      similarity = util.cos_sim(embeddings[i-1], embeddings[i])\n",
        "      if similarity < threshold or len(current_chunk) >= max_sentences:\n",
        "        chunks.append('. '.join(current_chunk))\n",
        "        current_chunk=[]\n",
        "      current_chunk.append(sentences[i])\n",
        "  if current_chunk:\n",
        "    chunks.append('. '.join(current_chunk))\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "zu5tGSeZqsC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}